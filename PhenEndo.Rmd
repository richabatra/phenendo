---
title: "PhenEndo"
author: "Richa Batra"
date: "June 30, 2018"
output:
  html_document:
    theme: readable
    highlight: zenburn
    toc: true
    toc_depth: 2
    number_sections: true
    #toc_float:
    #  collapsed: false
    #  smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages({library(StatMatch) # gower.dist
library(pheatmap) # pheatmap
library(NbClust) # cindex
library(FactoMineR) # famd
library(tidyverse) # %>% functions
library(dendextend) # dendrogram
library(cluster) # PAM
library(Rtsne) # tsne
library(ggplot2)
library(glmnet) # ridge regression
library(pvclust) # robust clusters
})

cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```


# Endotyping with R Markdown

This is an R Markdown document **to study endotypes targeted for questionaire or other biomedical records**. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. 

```{r input, echo=FALSE}
# the input data is already imputed for missing values using R package MICE
load("sampleData.RData")
source("srcFunctions.R")
```

# Data visualization {.tabset .tabset-fade .tabset-pills}

## Heatmap

```{r raw data heatmap, echo=FALSE}
#Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
# scaled the numeric variables to have the same range as categorical variables
plotMat <- standardizeMixedData(quesData)
pheatmap(data.matrix(plotMat), cluster_rows = F, cluster_cols = F, scale = "column")

```

## Each attribute
**Interpretation** in many of these plots one can see the imbalance in the binary variables, for instance attribute_1, attribute_4, attribute_7 and so on. These highly biased sets add to the noise in the data. On the other hand, if they co-occur they can be the deciding factor for the clustering algorithms. See next section on FAMD.
```{r raw data distribution, echo=FALSE}
par(mfrow=c(2,2))

lens <- lapply(1:ncol(quesData), FUN=function(i){
lens <- length(which(is.na(quesData[, i])==F | quesData[, i]!=""))
if(is.numeric(quesData[, i])){
 try(hist(quesData[, i], xlab="", main=paste0(names(quesData)[i]," : "
 , length(which(is.na(quesData[, i])==F | quesData[, i]!="")))))
} else if (is.factor(quesData[, i])){
try(barplot(table(quesData[, i]), ylab="Frequency", main=paste0(names(quesData)[i]," : "
 , length(which(is.na(quesData[, i])==F | quesData[, i]!="")))))
 }
return(lens)
})
```

## FAMD
Here the plot with qualitative varibles is particularly interesting. If you notice that many of the levels on the right are with value 1. Showing that many attributes are dependent and presence of a certain attribute co-occur with other attributes. 

```{r raw data pca, echo=FALSE}
plotMat <- nameMixedData (quesData)
quesFAMD <- FAMD(plotMat, graph = F)
plot(quesFAMD, choix="ind", title="Individual graph")
plot(quesFAMD, choix="quanti", title="Quantitative variables")
test <- plot(quesFAMD, choix="quali", title="Qualitative variables")
plot(quesFAMD, choix="var")
```

# Determine number of clusters {.tabset .tabset-fade .tabset-pills}

To determine the clusters we used two approaches. (a) C-index (CI) which is recommended for mixed data types**[cite]**. It is based on similarities, so we expect the lower the c-index the better the withing cluster similarity.(b) Silhouette width (SW) the most popular internal index. It is based on distances, ergo the higher the SW the better separation between clusters.

```{r kOpt, echo=FALSE}
quesDist <- gower.dist(quesData)
suppressWarnings({cindexOpt <- NbClust(data=NULL, diss=as.dist(quesDist), distance = NULL, min.nc = 2, max.nc = 15, method = "ward.D2", index = "cindex")})

numClusters <- names(cindexOpt$All.index)[which.min(cindexOpt$All.index)]

par(mfrow = c(1, 2))

plot(names(cindexOpt$All.index), cindexOpt$All.index, ylim=c(0, 1), xlab="number of clusters", ylab="c-index (CI)", main=paste0("CI minima at ",numClusters))

silOpt <- NbClust(data=NULL, diss=as.dist(quesDist), distance = NULL, min.nc = 2, max.nc = 15, method = "ward.D2", index="silhouette")

numSilClusters <- names(silOpt$All.index)[which.max(silOpt$All.index)]

plot(names(silOpt$All.index), silOpt$All.index, ylim=c(0, 1), xlab="number of clusters", ylab="silhouette width (SW)", main=paste0("SW maxima at ", numSilClusters))

```

# Clustering  {.tabset .tabset-fade .tabset-pills}

## Hierarchical Clustering
Here we use both the optimal number of clusters and observe. Two clusters as indicated by SW is very clear. However, the 5 clusters from CI is also represent true structure of the data. Thus, user can decide at which level of information they prefer to describe their data. For further demostrations, we have, for simplicity, chosen cluster number by CI.

```{r hclust, echo=FALSE}
hclusters <- hclust(as.dist(quesDist), method="ward.D2")
hclusIDs <- cutree(hclusters, k=5)

dend <- as.dendrogram(hclusters)
dend %>%
  set("labels_col", value = cbPalette[1:numSilClusters], k=numSilClusters) %>%
  set("branches_k_color", value = cbPalette[1:numSilClusters], k = numSilClusters) %>%
  plot(horiz=TRUE, axes=FALSE, main="k optimized by silhouette width")
abline(v = 350, lty = 2)

dend %>%
  set("labels_col", value = cbPalette[1:numClusters], k=numClusters) %>%
  set("branches_k_color", value = cbPalette[1:numClusters], k = numClusters) %>%
  plot(horiz=TRUE, axes=FALSE, main = "k optimized by c-index")
abline(v = 350, lty = 2)
```

## Partition Around Mediods (PAM)

```{r pam, echo=FALSE}
pamfit <- pam(quesDist, k = numClusters)
pamfitSil <- pam(quesDist, k = numSilClusters)

mdsfit <- cmdscale(quesDist, eig = TRUE, k = 2)
MDSdim1 <- mdsfit$points[, 1]
MDSdim2 <- mdsfit$points[, 2]
par(mfrow=c(2,2))
plot(MDSdim1, MDSdim2, pch = 19, col=cbPalette[pamfit$clustering], main="k optimized by c-index")
plot(MDSdim1, MDSdim2, pch = 19, col=cbPalette[pamfitSil$clustering], main="k optimized by silhouette width")
```

## Robustness analysis
Clustering is an unsupervised approach. There is no cross validation to tell us the errors in our model. Thus, we use boostrap approach as implemented in pvclust to observe the robustness in the groups. It shows the dynamics of clusters when a set of samples is removed from the dataset or added to the dataset. 

At the same time, most of the times the clusters do not pass this robustness test. It can be attributed to many reasons like low sample size to high overall similarity between the samples. Nevertheless, it is worth trying. 

Note: please change the nboot value to 1K to 10K for good evaluation. 

```{r pvclust, echo=FALSE}

gowerDist <- function(mat){
  as.dist(gower.dist(data.frame(t(mat))))
}

pvcfit <- pvclust(t(quesData), method.hclust="ward.D2",
               method.dist=gowerDist, nboot = 10) # recommend 1K to 10K
plot(pvcfit, hang=-1) # dendrogram with p values
# add rectangles around groups highly supported by the data
pvrect(pvcfit, alpha=.8)
```

# Non-linear visualization of the clusters {.tabset .tabset-fade .tabset-pills}

With different perplexity values, one can observe the cluster structure taking shape. 

```{r tsne, echo=FALSE}

perps <- c(1, 3, 5, 10, 20, 30, 40)
# tsen on distance matrix
plotMats <- lapply(1:length(perps), FUN=function(p){
  tsne_obj <- Rtsne(quesDist, is_distance = TRUE,perplexity=perps[p], verbose=F, max_iter=10000 )
  tsne_data <- tsne_obj$Y %>%
    data.frame() %>%
    setNames(c("X", "Y")) %>%
    mutate(cluster = factor(pamfit$clustering)) %>%
    mutate(perplexity = perps[p])
})

plotMat <- do.call(rbind, plotMats)
ggplot(aes(x = X, y = Y), data = plotMat) +
  geom_point(aes(color = cluster))+
  facet_wrap(~ perplexity, scales = "free") +
  theme_bw() +
  theme(axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank())

```

# Cluster Signature 

## With Questionaire data {.tabset .tabset-fade .tabset-pills}

### Multivariate regression

```{r quesRidge, echo=FALSE}

cluster.ids <- pamfit$clustering
design.mat <- model.matrix(cluster.ids ~ data.matrix(quesData))
glmfit <- glmnet(design.mat, cluster.ids, family = "multinomial", alpha = 0)
glmfit.cv <- cv.glmnet(design.mat, cluster.ids, family = "multinomial")
coef.mat <- do.call(cbind, coef(glmfit, s=glmfit.cv$lambda.1se))

plotMat <- coef.mat[-c(1:2), ]
rownames(plotMat) <- names(quesData)
colnames(plotMat) <- 1:numClusters
pheatmap(plotMat, cluster_rows = F, cluster_cols = F, display_numbers = T )
```

### Univariate hypothesis testing
```{r expHypo, echo=FALSE}
```

## With expression data {.tabset .tabset-fade .tabset-pills}

### Multivariate regression
```{r exprRidge, echo=FALSE}

cluster.ids <- pamfit$clustering
design.mat <- model.matrix(cluster.ids ~ data.matrix(expData))
glmfit <- glmnet(design.mat, cluster.ids, family = "multinomial", alpha = 0)
glmfit.cv <- cv.glmnet(design.mat, cluster.ids, family = "multinomial")
coef.mat <- do.call(cbind, coef(glmfit, s=glmfit.cv$lambda.1se))

plotMat <- coef.mat[-c(1:2), ]
rownames(plotMat) <- names(expData)
colnames(plotMat) <- 1:numClusters
pheatmap(plotMat, cluster_rows = F, cluster_cols = F, display_numbers = T )
```

### Univariate hypothesis testing
```{r exprHypo, echo=FALSE}
```

<!-- # External validation -->
<!-- ```{r hyperG, echo=FALSE} -->

<!-- hyperTest <- lapply(1:5, FUN=function(i) { -->
<!--  clusOne <- which(pamfit$clustering==i) -->
<!--   a <- length(which(patGroups[clusOne]==1)) -->
<!--   c <- length(which(patGroups[clusOne]==0)) -->
<!--   b <- length(which(patGroups==1))-a -->
<!--   d <- length(which(patGroups==0))-c -->
<!--  ftest <- fisher.test(matrix(c(a, b, c, d), byrow = T, nrow = 2), alternative = "two.sided") -->
<!--   q <- a -->
<!--   m <- length(which(patGroups==1)) -->
<!--   n <- length(which(patGroups==0)) -->
<!--   k <- length(clusOne) -->
<!--   hyperLower <- phyper(q, m , n, k, lower.tail = F) -->
<!--   hyperUpper <- phyper(q, m , n, k, lower.tail = T) -->

<!--   return(list(hyperLower, hyperUpper)) -->
<!-- }) -->
<!-- ``` -->