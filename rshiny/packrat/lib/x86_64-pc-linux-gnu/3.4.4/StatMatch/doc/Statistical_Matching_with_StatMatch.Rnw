\documentclass[a4paper,11pt]{scrartcl}

\usepackage{Sweave}
\SweaveOpts{keep.source=TRUE}
% \VignetteIndexEntry{Examples on using StatMatch to integrate two data sources}

\usepackage[OT1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\hypersetup{colorlinks, citecolor=blue, linkcolor=blue, urlcolor=blue}
%\usepackage[top=35mm, bottom=30mm, left=35mm, right=30mm]{geometry}

%% additional commands
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\mbox{\textbf{#1}}}
\newcommand{\proglang}[1]{\mbox{\textsf{#1}}}
\newcommand{\dQuote}[1]{``#1''}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Statistical Matching and Imputation of Survey Data with \pkg{StatMatch}\footnote{This document is partly based on the work carried out in the framework of the ESSnet project on Data Integration, partly funded by Eurostat (December 2009--December 2011).  For more information on the project visit \url{http://www.essnet-portal.eu/di/data-integration}}
}
\author{
  Marcello D'Orazio\\
%   \large \emph{Italian National Institute of Statistics (Istat), Rome, Italy}\\
   \large E-mail: \href{mailto:mdo.statmatch@gmail.com}{mdo.statmatch@gmail.com}
}

\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle

% ------------
% introduction
% ------------

\section{Introduction} \label{sec:intro}

Statistical matching techniques aim at integrating two or more data sources (usually data from sample surveys) referred to the same target population.  In the basic statistical matching  framework, there are two data sources $A$ and $B$ sharing a set of variables $X$ while the variable $Y$ is available only in $A$ and the variable $Z$ is observed just in $B$.  The $X$ variables are common to both the data sources, while the variables $Y$ and $Z$ are not jointly observed.  The objective of statistical matching (hereafter denoted as SM) consists in investigating the relationship between $Y$ and $Z$ at \dQuote{micro} or \dQuote{macro} level (D'Orazio \emph{et al.}, 2006b).  In the micro case the SM aims at creating a \dQuote{synthetic} data source in which all the variables, $X$, $Y$ and $Z$, are available (usually $A \cup B$ with all the missing values filled in, or simply $A$ filled in with the values of $Z$).  When the objective is macro, the data sources are integrated to derive an estimate of the parameter of interest, e.g. the correlation coefficient between $Y$ and $Z$ or the contingency table $Y \times Z$.  

A parametric approach to SM requires the explicit adoption of a model for $(X,Y,Z)$; obviously, if the model is misspecified the results will not be reliable.  The nonparametric approach is more flexible in handling complex situations (different types of variables).  The two approaches can be mixed: first a parametric model is assumed and its parameters are estimated then a synthetic data set is derived through a nonparametric micro approach.  In this manner the advantages of both parametric and nonparametric approach are maintained: the model is parsimonious while nonparametric techniques offer protection against model misspecification.  Table \ref{SMapproaches} provides a summary of the objectives and approaches to SM (D'Orazio \emph{et al.}, 2008).

\begin{table}
\begin{center}
\caption{Objectives and approaches to Statistical matching.}
\begin{tabular}{c|ccc}
 \hline
 Objectives of & \multicolumn{3}{c}{Approaches to statistical Matching} \\
 Statistical matching & Parametric & Nonparametric & Mixed \\
\hline
  MAcro & yes & yes & no \\
  MIcro & yes & yes & yes \\
\hline
\end{tabular}
\label{SMapproaches}
\end{center}
\end{table}

In the traditional SM framework when only $A$ and $B$ are available, all the SM methods (parametric, nonparametric and mixed) that use the set of common variables $X$ to match $A$ and $B$, implicitly assume the \emph{conditional independence} (CI) of $Y$ and $Z$ given $X$:

$$ 
f\left( x,y,z \right)=f \left( y|x \right) \times f\left( z|x \right) \times f\left( x \right)
$$

This assumption is particularly strong and seldom holds in practice.  To avoid it the SM should incorporate some auxiliary information concerning the relationship between $Y$ and $Z$ (see Chap. 3 in D'Orazio \emph{et al.} 2006b).  The auxiliary information can be at micro level (a new data source in which $Y$ and $Z$ or $X$, $Y$ and $Z$ are jointly observed) or at macro level (e.g. an estimate of the correlation coefficient $\rho_{XY}$ or an estimate of the contingency table $Y \times Z$, etc.) or simply consist of some logic constraints about the relationship between $Y$ and $Z$ (structural zeros, etc.; for further details see D'Orazio \emph{et al.}, 2006a).

An alternative approach to SM consists in the evaluation of the \emph{uncertainty} when estimating the parameters of interest.  This uncertainty is due to the lack of joint information concerning $Y$ and $Z$.  For instance, let us assume that $(X,Y,Z)$ follows a trivariate normal distribution and the goal of SM consists in estimating the correlation matrix; in the basic SM framework the available data allow to estimate all the components of the correlation matrix with the exception of $\rho_{YZ}$; in this case, due to the properties of the correlation matrix (has to be semidefinite positive), it is possible to conclude that:

$$
\rho_{XY} \rho_{XZ} - \sqrt{\left( 1 - \rho_{YX}^2\right) \left( 1 - \rho_{XZ}^2\right)} 
\leq \rho_{YZ} \leq 
\rho_{XY} \rho_{XZ} + \sqrt{\left( 1 - \rho_{YX}^2\right) \left( 1 - \rho_{XZ}^2\right)}
$$

The higher is the correlation between $X$ and $Y$ and between $X$ and $Z$, the shorter will be the interval and, consequently, the lower will be the uncertainty.  In practical applications, by substituting the correlation coefficients with the corresponding estimates it is possible to derive a \dQuote{range} of admissible values of the unknown $\rho_{YZ}$.  The investigation of the uncertainty in SM will be discussed in the Section \ref{sec:unc}.

Section \ref{sec:SMapplication} will be discuss some practical aspects concerning the preliminary steps, with emphasis on the choice of the marching variables;  moreover some example data will be introduced.  In Section \ref{sec:npmicro} some nonparametric approaches to SM at micro will be shown. Section \ref{sec:mix} is devoted to the mixed approaches to SM.  Section \ref{sec:comp_survey} will discuss SM approaches to deal with data arising from complex sample surveys carried out on finite populations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Application of SM

\section{Practical steps in an application of statistical matching} \label{sec:SMapplication}

Before applying SM methods in order to integrate two or more data sources some decisions and preprocessing steps are required (Scanu, 2008).  In practice, given two data sources $A$ and $B$ related to the same target population, the following steps are necessary:

\begin{enumerate}
	\item Choice of the target variables $Y$ and $Z$, i.e. of the variables observed distinctly in two sample surveys.
	\item Identification of all the common variables $X$ shared by $A$ and $B$.  In this step some harmonization procedures may be required because of different definitions and/or classifications.  Obviously, if two similar variables can not be harmonized they have to be discarded.  The common variables should not present missing values and the observed values should be accurate (no measurement errors). Note that if $A$ and $B$ are representative samples of the same population then the common variables are expected to share the same marginal/joint distribution. 
	\item Potentially all the $X$ variables can be used directly in the SM application, so called \emph{matching variables} but actually, not all them are used.  Section \ref{sec:mtc_vars} will provide more details concerning this issue.
	\item The choice of the matching variables is strictly related to the matching framework (see Table \ref{SMapproaches}).
	\item Once decided the framework, a SM technique is used to match the samples.
	\item Finally the results of the matching should be evaluated.
	
\end{enumerate}

%%%%%%%%%%%%%%%%%
% Example data

\subsection{Example data} \label{sec:data}
The next Sections will provide simple examples of application of some SM techniques in the \proglang{R} environment (R Core Team, 2015) by using the functions in \pkg{StatMatch} (D'Orazio, 2016).  These examples will use artificial data set that come with \pkg{StatMatch}; these artificial data sets are generated by considering the variables usually observed in the EU-SILC (European Union Statistics on Income and Living Conditions) survey (for major details see \pkg{StatMatch} help pages).

% change some options
<<echo=FALSE, results=hide>>=
options(useFancyQuotes="UTF-8")
#options(useFancyQuotes=FALSE)
options(width=66)
options(warn=-1)
@


<<>>=
library(StatMatch) #loads pkg StatMatch
data(samp.A) # sample A in SM examples
str(samp.A)

data(samp.B) # sample B in the SM examples
str(samp.B)
@

The two data frames \code{samp.A} and \code{samp.B} share the variables \code{X.vars}; the net income (\code{z.var}) is available in \code{samp.A} while the person's economic status (\code{y.var}) is available only in \code{samp.B}.

<<>>=
X.vars <- intersect(names(samp.A), names(samp.B))
X.vars

setdiff(names(samp.A), names(samp.B)) # available just in A
setdiff(names(samp.B), names(samp.A)) # available just in B
@

For major details on the variables contained in $A$ and $B$ see the corresponding help pages.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The choice of the matching variables} \label{sec:mtc_vars}

In statistical matching applications $A$ and $B$ may share many common variables.  In practice, just the most relevant ones, called \emph{matching variables}, are used in the matching.  The selection of these variables should be performed through opportune statistical methods (descriptive, inferential, etc.) and by consulting subject matter experts.

From a statistical point of view, the choice of the marching variables $X_M$ $(X_M \subseteq X)$ should be carried out in a \dQuote{multivariate sense} in order to identify the subset of the $X_M$ variables connected at the same time with $Y$ and $Z$ (Cohen, 1991); unfortunately this would require the availability of an auxiliary data source in which all the variables $(X,Y,Z)$ are observed.  In the basic SM framework the data in $A$ permit to explore the relationship between $Y$ and $X$, while the relationship between $Z$ and $X$ can be investigated in $B$.  Then the results of the two separate analyses have to be combined in some manner; usually the subset of the matching variables is obtained as $X_M = X_Y \cup X_Z$, being $X_Y$ $(X_Y \subseteq X)$ the subset of the common variables that better explains $Y$, while $X_Z$ is the subset of the common variables that better explain $Z$ $(X_Z \subseteq X)$.  The risk is that of ending with too many matching variables, thereby increasing the complexity of the problem and potentially affecting negatively the results of SM.  In particular, in the micro approach this may introduce additional undesired variability and bias as far as the joint (marginal) distribution of $X_M$ and $Z$ is concerned.  For this reason sometimes the set of the matching variables is obtained as a compromise

$$
  X_Y \cap X_Z  \subseteq X_M \subseteq X_Y \cup X_Z
$$.  

The simplest procedure to identify $X_Y$ consists in calculation of pairwise correlation/association measures between $Y$ and each of the available predictors $X$.  The same analysis should be performed on $B$ to identify the best predictors od $Z$.  When the response variable is continuous one can look at correlation with the predictors. In order to identify eventual nonlinear relationship it may be convenient to consider the ranks (Spearman's rank correlation coefficient).  An interesting suggestion from Harrell (2001) consists in looking at the adjusted $R^2$ related to the regression  model rank($Y$)~vs.~rank($X$) (unadjusted $R^2$ corresponds to squared Spearman's rank correlation coefficient).  When $X$ is categorical nominal variable it is considered the adjusted $R^2$ of the regression model rank($Y$)~vs.~dummies($X$).  The function \code{spearman2} in the package \pkg{Hmisc} (Harrell \emph{et al.}, 2014) computes automatically the adjusted $R^2$ for each couple response-predictor.

<<>>=
require(Hmisc)
spearman2(n.income~area5+urb+hsize+age+sex+marital+edu7, 
          p=2, data=samp.A)
@

By looking at the adjusted  $R^2$, it comes out that just the gender (\code{sex}) and age (\code{age}) have a certain predictive power on \code{n.income}. 

When response and predictors are all categorical, then Chi-square based association measures (Cramer's $V$) or \textit{proportional reduction of the variance} measures can be considered.  The function \code{pw.assoc} in \pkg{StatMatch} computes some of them.

<<>>=
pw.assoc(labour5~area5+urb+hsize5+c.age+sex+marital+edu7, data=samp.B)
@

In practice it comes out the best predictor of person's economic status (\code{labour5}) is the age conveniently categorized (\code{c.age}), among the remaining variables, just the education levels (\code{edu7}) has some a certain predictive power on \code{labour5}.

To summarize, in this example the set of the matching variables could be composed by \code{age}, \code{sex} and \code{edu7} (i.e. $X_M = X_Y \cup X_Z $).

When too many variables are available, before computing pairwise association/correlation measures it would be necessary to discard the redundant predictors (functions \code{redun} and \code{varclus} in \pkg{Hmisc} can be of help).  

Sometimes the important predictors can be identified by fitting models and then running procedures for selecting the best predictors. The selection of the subset $X_Y$ can also be demanded to nonparametric procedures such as \emph{Classification And Regression Trees} (Breiman \emph{et al.}, 1984).  Instead of fitting a single tree, it would be better to fit a \emph{random forest} (Breiman, 2001) by means of the function \code{randomForest} available in the package \pkg{randomForest} (Liaw and Wiener, 2002) which provides a measure of importance for the predictors (to be used with caution).

The approach to SM based on the study of uncertainty offers the possibility of choosing the matching variable by selecting just those common variables with the highest contribution to the reduction of the uncertainty.  The function \code{Fbwidths.by.x} in \pkg{StatMatch} permits to explore the reduction of uncertainty when all the variables $(X,Y,Z)$ are categorical.  In particular, assuming that $X_D$ correspond to the complete crossing of the matching variables $X_M$, it is possible to show that in the basic SM framework  

$$
P^{(low)}_{j,k} \leq P_{Y=j,Z=k} \leq P^{(up)}_{j,k},
$$

\noindent being

\begin{eqnarray}
P^{(low)}_{j,k} &=& \sum_{i}  P_{X_D=i} \times \max \left\{0; P_{Y=j|X_D=i} + P_{Z=k|X_D=i} - 1 \right\} \nonumber\\
P^{(up)}_{j,k} &=& \sum_{i}  P_{X_D=i} \times  \min \left\{P_{Y=j|X_D=i}; P_{Z=k|X_D=i}\right\} \nonumber
\end{eqnarray}

\noindent for $j=1,\ldots, J$ and $k=1,\ldots, K$, being $J$ and $K$ the categories of $Y$ and $Z$ respectively.

The function \code{Fbwidths.by.x} estimates $(P^{(low)}_{j,k},P^{(up)}_{j,k})$ for each cell in the contingency table $Y \times Z$ for all the possible combinations of the input $X$ variables; then the reduction of uncertainty is measured naively by the average widths of the intervals:

$$
\bar{d} = \frac{1}{J \times K} \sum_{j,k} ( \hat{P}^{(up)}_{j,k} - \hat{P}^{(low)}_{j,k} )
$$


%according to the proposal of Conti \emph{et al.} (2012):

%$$
%\hat{\Delta} = \sum_{i,j,k} \left( \hat{P}^{(up)}_{j,k} - \hat{P}^{(low)}_{j,k} \right) \times \hat{P}_{Y=j|X_D=i} \times \hat{P}_{Z=k|X_D=i} \times \hat{P}_{X_D=i}
%$$


<<>>=
# choiche of the matching variables based on uncertainty
xx <- xtabs(~c.age+sex+marital+edu7, data=samp.A)
xy <- xtabs(~c.age+sex+marital+edu7+c.neti, data=samp.A)
xz <- xtabs(~c.age+sex+marital+edu7+labour5, data=samp.B)

out.fbw <-  Fbwidths.by.x(tab.x=xx, tab.xy=xy, tab.xz=xz)

# sort output according to average width
sort.av <- out.fbw$sum.unc[order(out.fbw$sum.unc$av.width),]
head(sort.av) # best 6 combinations of the Xs
@

By looking at the average width of the cells bounds it appears that all $X$s being considered should be used as matching variables. Unfortunately the input tables \code{tab.xy} and \code{tab.xy} have a lot of empty cells (cells with frequency equal to zero; 863 out of 1470 in \code{tab.xy} and 538 out of 1050 in \code{tab.xy}); in such a context the results may be unreliable. On the contrary, the second best choice of $X$ variables, even if determines a slight worsening of the average width of intervals, seems a viable alternative, even because the input tables are much less sparse; moreover the $X$ variables are the same that would be chosen as matching variables at the end of the previous analyses based on computations of pairwise association/correlations measures.  Note that in the presence of sparse contingency tables a possible alternative way of working may consist in estimating the cells probabilities by applying a pseudo-Bayes estimator, implemented in the function \code{pBayes} available in \pkg{StatMatch} (see corresponding help pages for major details).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Nonparametric micro techniques

\section{Nonparametric micro techniques} \label{sec:npmicro}

Nonparametric approach is very popular in SM when the objective is the creation of a synthetic data set.  Most of the nonparametric micro approaches consists in filling in the data set chosen as the \emph{recipient} with the values of the variable which is available only in the other data set, the \emph{donor} one.  In this approach it is important to decide which data set plays the role of the recipient; usually it is the data set to be used as the basis for further statistical analyses.  The obvious choice would be that of using the larger one because it is expected to provide more accurate estimates; unfortunately, such a way of working may provide inaccurate SM results, especially when the sizes of the two data sources are very different.  In practice, the larger is the recipient with respect to the donor, the more times a unit in the latter could be selected as a donor.  In this manner, there is a high risk that the distribution of the imputed variable does not reflect the original one (estimated form the donor data set).  In the following it will be assumed that $A$ is the recipient while $B$ is the donor, being $n_A \leq n_B$ ($n_A$ and $n_B$ are the sizes of $A$ and $B$ respectively).  Hence the objective of SM will be that of filling in $A$ with values of $Y$ (variable available only in $B$).

In \pkg{StatMatch} the following nonparametric micro techniques are available: \emph{random hot deck}, \emph{nearest neighbor hot deck} and \emph{rank hot deck} (see Section 2.4 in D'Orazio \emph{et al.}, 2006b; Singh \emph{et al.}, 1993).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Nearest neighbor distance hot deck} \label{sec:nnd}

The nearest neighbor distance hot deck techniques are implemented in the function \code{NND.hotdeck}.  This function searches in \code{data.don} the nearest neighbor of each unit in \code{data.rec} according to a distance computed on the matching variables $X_M$ specified with the argument \code{match.vars}.  By default the Manhattan (city block) distance is considered (\code{dist.fun="Manhattan"}).  In order to reduce the effort in computing distances it is preferable to define some donation classes (argument \code{don.class}): for a record in given donation class it will be selected a donor in the same class (the distances are computed only between units belonging to the same class).  Usually, the donation classes are defined according to one or more categorical common variables (geographic area, etc.).  In the following, a simple example of usage of \code{NND.hotdeck} is reported; donation classes are formed using large geographical areas (\code{"area5"}) and gender (\code{"sex"}), while distances are computed on age (\code{"age"}):

<<>>=
group.v <- c("area5","sex")
X.mtc <- "age" 
out.nnd <- NND.hotdeck(data.rec=samp.A, data.don=samp.B,
                       match.vars=X.mtc, don.class=group.v)
@

The function \code{NND.hotdeck} does not create the synthetic data set; for each unit in $A$  the corresponding closest donor in $B$ is identified according to the imputation classes (when defined) and the chosen distance function; the recipient-donor units' identifiers are saved in the data.frame \code{mtc.ids} stored in the output list returned by \code{NND.hotdeck}.  This list provides also the distance between each couple recipient-donor (saved in the \code{dist.rd} component of the output list) and the number of available donors at the minimum distance for each recipient (component \code{noad}).  Note that when there are more donors at the minimum distance, then one of them is picked up at random.

<<>>=
summary(out.nnd$dist.rd) # summary distances rec-don
summary(out.nnd$noad) # summary available donors at min. dist.
@

In order to derive the synthetic data set it is necessary to run the function \code{create.fused} which requires the output of \code{NND.hotdeck} function (component \code{mtc.ids} of the output's list) and the specification of the $Z$ variables to donate from $B$ to $A$ via the argument \code{z.vars}:

<<>>=
head(out.nnd$mtc.ids)
fA.nnd <- create.fused(data.rec=samp.A, data.don=samp.B,
                       mtc.ids=out.nnd$mtc.ids,
                       z.vars="labour5")

head(fA.nnd) #first 6 obs.
@

As far as distances are concerned (argument \code{dist.fun}), all the distance functions in the package \pkg{proxy} (Meyer and Butchta, 2015) are available.  Anyway, for some particular distances it was decided to write specific \proglang{R} functions.  In particular, when dealing with continuous matching variables it is possible to use the \emph{maximum distance} ($L^{\infty}$ norm) implemented in \code{maximum.dist}; this function works on the true observed values (continuous variables) or on transformed ranked values (argument \code{rank=TRUE}) as suggested in Kovar \emph{et al.} (1988); the transformation (ranks divided by the number of units) removes the effect of different scales and the new values are uniformly distributed in the interval $[0,1]$.  The Mahalanobis distance can be computed by using \code{mahalanobis.dist} which allows an external estimate of the covariance matrix (argument \code{vc}).  When dealing with mixed type matching variables, the \emph{Gowers's dissimilarity} (Gower, 1981) can be computed (function \code{gower.dist}): it is an average of the distances computed on the single variables according to different rules, depending on the type of the variable.  All the distances are scaled to range from 0 to 1, hence the overall distance cat take a value in $[0,1]$.  When dealing with mixed types matching variables it is still possible to use the distance functions for continuous variables but \code{NND.hotdeck} transforms factors into dummies (by means of the function \code{fact2dummy}).

By default \code{NND.hotdeck} does not pose constraints on the \dQuote{usage} of donors: a record in the donor data set can be selected many times as a donor.  The multiple usage of a donor can be avoided by resorting to a \emph{constrained hot deck} (argument \code{constrained=TRUE} in \code{NND.hotdeck}); in such a case, a donor can be used just once and all the donors are selected in order to minimize the overall matching distance.  In practice, the donors are identified by solving a traveling salesperson problem; two alternatives are available: the Hungarian algorithm (argument \code{constr.alg="Hungarian"} implemented in the function \code{solve\_LSAP} in the package \pkg{clue} (Hornik, 2005 and 2015) and the algorithm provided by the package \pkg{lpSolve} (Berkelaar \emph{et al.}, 2015) (argument \code{constr.alg="lPsolve"}).  Setting \code{constr.alg="Hungarian"} (default) is more efficient and faster. 

<<>>=
group.v <- c("sex","area5")
X.mtc <- "age"
out.nnd.c <- NND.hotdeck(data.rec=samp.A, data.don=samp.B, 
                         match.vars=X.mtc, don.class=group.v, 
                         dist.fun="Manhattan", constrained=TRUE, 
                         constr.alg="Hungarian")
fA.nnd.c <- create.fused(data.rec=samp.A, data.don=samp.B,
                         mtc.ids=out.nnd.c$mtc.ids,
                         z.vars="labour5")
@

The constrained matching returns an overall matching distance greater than the one in the unconstrained case, but it tends to better preserve the marginal distribution of the variable imputed in the synthetic data set.  

<<>>=
#comparing distances
sum(out.nnd$dist.rd) # unconstrained
sum(out.nnd.c$dist.rd) # constrained
@

To compare the marginal joint distributions of a set of categorical variables it is possible to resort to the function \code{comp.prop} in \pkg{StatMatch} which provides some similarity measure among distributions of categorical variables and performs also the Chi-square test (for details see \code{comp.prop} the help pages).  

<<>>=
# estimating marginal distribution of labour5
tt0 <- xtabs(~labour5, data=samp.B) # reference distr.
tt <- xtabs(~labour5, data=fA.nnd)  # synt unconstr.
ttc <- xtabs(~labour5, data=fA.nnd.c) #synt. constr.
#
# comparing marginal distributions
cp1 <- comp.prop(p1=tt, p2=tt0, n1=nrow(fA.nnd), n2=NULL, ref=TRUE)
cp2 <- comp.prop(p1=ttc, p2=tt0, n1=nrow(fA.nnd), n2=NULL, ref=TRUE)
cp1$meas
cp2$meas
@

By looking at \code{comp.prop} output it comes out that, as expected, the marginal distribution of \code{c.netI} in the synthetic file obtained after constrained NND is closer to the reference distribution (estimated on the donor dataset) than the one estimated from the synthetic file after the unconstrained NND.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Random hot deck} \label{sec:rand}

The function \code{RANDwNND.hotdeck} carries out the random selection of each donor from a suitable subset of all the available donors.  This subset can be formed in different ways, e.g. by considering all the donors sharing the same characteristics of the recipient (defined according to some $X_M$ variables, such as geographic region, etc.).  The traditional \emph{random hot deck} (Singh \emph{et al.}, 1993) within imputation classes is performed by simply specifying the donation classes via the argument \code{don.class} (the classes are formed by crossing the categories of the categorical variables being considered).  For each recipient record in a given donation class, a donor is picked up completely at random within the same donation class.

<<>>=
# random hot deck in classes formed crossing "area5" and "sex"
group.v <- c("area5","sex")
rnd.1 <- RANDwNND.hotdeck(data.rec=samp.A, data.don=samp.B, 
                          match.vars=NULL, don.class=group.v)
fA.rnd <- create.fused(data.rec=samp.A, data.don=samp.B,
                       mtc.ids=rnd.1$mtc.ids, 
                       z.vars="labour5")
@

As for \code{NND.hotdeck}, the function \code{RANDwNND.hotdeck} does not create the synthetic data set; the recipient-donor units' identifiers are saved in the component \code{mtc.ids} of the list returned in output.  The number of donors available in each donation class are saved in the component \code{noad}.

\code{RANDwNND.hotdeck} implements various alternative methods to create classes of donors by using a continuous matching variable.  These methods are based essentially on a distance measure computed on the matching variables provided via the argument \code{match.vars}.  In practice, when \code{cut.don="k.dist"} only the donors whose distance from the recipient is less or equal to threshold \code{k} are considered (see Andridge and Little, 2010).  By setting \code{cut.don="exact"} the \code{k} $(0 < k \leq n_D)$ closest donors are retained ($n_D$ is the number of available donors for a given recipient).  With \code{cut.don="span"} a proportion \code{k} $(0 < k \leq 1)$ of the closest available donors it is considered; while, setting \code{cut.don="rot"} and \code{k=NULL}  the subset reduces to the $\left[ \sqrt{n_D}\right]$ closest donors;  finally, when \code{cut.don="min"} only the donors at the minimum distance from the recipient are retained.

<<>>=
# random choice of a donor among the closest k=20 wrt age
# sharing the same values of "area5" and "sex"
group.v <- c("area5","sex")
X.mtc <- "age"
rnd.2 <- RANDwNND.hotdeck(data.rec=samp.A, data.don=samp.B, 
                          match.vars=X.mtc, don.class=group.v, 
                          dist.fun="Manhattan", 
                          cut.don="exact", k=20)
fA.knnd <- create.fused(data.rec=samp.A, data.don=samp.B,
                        mtc.ids=rnd.2$mtc.ids, 
                        z.vars="labour5")
@

When distances are computed on some matching variables, then the output of \code{RANDwNND.hotdeck} provides some information concerning the distances of the possible available donors for each recipient observation.

<<>>=
head(rnd.2$sum.dist)
@

In particular, \code{"min"}, \code{"max"} and \code{"sd"} columns report respectively the minimum, the maximum and the standard deviation of the distances (all the available donors are considered), while \code{"cut"} refers to the distance of the \code{k}th closest donor; \code{"dist.rd"} is distance existing among the recipient and the randomly chosen donor.

When selecting a donor among those available in the subset identified by the arguments \code{cut.don} and \code{k}, it is possible to use a weighted selection by specifying a weighting variable via \code{weight.don} argument.  This topic will be tackled in Section \ref{sec:comp_survey}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Rank hot deck} \label{sec:rank}

The \emph{rank hot deck distance} method has been introduced by Singh \emph{et al.} (1993).  It searches for the donor at a minimum distance from the given recipient record but, in this case, the distance is computed on the percentage points of the empirical cumulative distribution function of the unique (continuous) common variable $X_M$ being considered.  The empirical cumulative distribution function is estimated by:

$$
\hat{F}(x) = \frac{1}{n} \sum_{i=1}^{n} I\left(x_i\leq x\right) 
$$

\noindent being $I()=1$ if $x_i\leq x$ and 0 otherwise.  This transformation provides values uniformly distributed in the interval $\left[0,1\right]$; moreover, it can be useful when the values of $X_M$ can not be directly compared because of measurement errors which however do not affect the \dQuote{position} of a unit in the whole distribution (D'Orazio \emph{et al.}, 2006b).  This method is implemented in the function \code{rankNND.hotdeck}.  The following simple example shows how to call it.

<<>>=
# distance computed on the percentage points of ecdf of "age"
rnk.1 <- rankNND.hotdeck(data.rec=samp.A, data.don=samp.B, 
                         var.rec="age", var.don="age")
#create the synthetic data set
fA.rnk <- create.fused(data.rec=samp.A, data.don=samp.B,
                       mtc.ids=rnk.1$mtc.ids, 
                       z.vars="labour5", 
                       dup.x=TRUE, match.vars="age")
head(fA.rnk)
@

The function \code{rankNND.hotdeck} allows for constrained and unconstrained matching in the same manner as in \code{NND.hotdeck}. It is also possible to define some donation classes (argument \code{don.class}), in this case the empirical cumulative distribution is estimated separately class by class.  

<<>>=
# distance computed on the percentage points of ecdf of "age"
# computed separately by "sex"
rnk.2 <- rankNND.hotdeck(data.rec=samp.A, data.don=samp.B, var.rec="age",
                         var.don="age", don.class="sex",
                         constrained=TRUE, constr.alg="Hungarian")
fA.grnk <- create.fused(data.rec=samp.A, data.don=samp.B,
                        mtc.ids=rnk.2$mtc.ids, 
                        z.vars="labour5",
                        dup.x=TRUE, match.vars="age")
head(fA.grnk)
@

In estimating the empirical cumulative distribution it is possible to consider the units' weights (arguments \code{weight.rec} and \code{weight.don}).  This topic will be tackled in Section \ref{sec:comp_survey}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% imputation of missing data
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Using functions in \pkg{StatMatch} to impute missing values in a survey} \label{sec:imp}

All the functions in \pkg{StatMatch} that implement the hot deck imputation techniques can be used to impute missing values in a single data set.  In this case it is necessary to:

\begin{enumerate}

	\item {separate the observations in two data sets: the file $A$ plays the role of recipient and will contain the units with missing values on the target variable, while the file $B$ is the donor and will contain all the available donors (units with non missing values for the target variable).}
	
	\item {Fill in the missing values in the recipient by means of an hot deck imputation technique.}
	
	\item {Join recipient and donor file.}
	
\end{enumerate}

  In the following a simple example with the \code{iris} data.frame is reported. Distance hot deck is used to fill missing values in the recipient. 

<<>>=
# step 0) introduce missing values in iris
data(iris, package="datasets")
set.seed(1324)
miss <- rbinom(150, 1, 0.30) #generates randomly missing
iris.miss <- iris
iris.miss$Petal.Length[miss==1] <- NA
summary(iris.miss$Petal.L)
#
# step 1) separate units in two data sets
rec <- subset(iris.miss, is.na(Petal.Length), select=-Petal.Length)
don <- subset(iris.miss, !is.na(Petal.Length))
#
# step 2) search for closest donors
X.mtc <- c("Sepal.Length", "Sepal.Width", "Petal.Width")
nnd <- NND.hotdeck(data.rec=rec, data.don=don,
                         match.vars=X.mtc, don.class="Species",
                         dist.fun="Manhattan")
# fills rec
imp.rec <- create.fused(data.rec=rec, data.don=don,
                        mtc.ids=nnd$mtc.ids, z.vars="Petal.Length")
imp.rec$imp.PL <- 1 # flag for imputed
#
# step 3) re-aggregate data sets
don$imp.PL <- 0
imp.iris <- rbind(imp.rec, don)
#summary stat of imputed and non imputed Petal.Length
tapply(imp.iris$Petal.Length, imp.iris$imp.PL, summary)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Mixed methods} \label{sec:mix}

A SM mixed method consists of two steps: (1) a model is fitted and all its parameters are estimated, then (2) a nonparametric approach is used to create the synthetic data set.  The model is more parsimonious while the nonparametric approach offers \dQuote{protection} against model misspecification.  The implemented mixed approaches for SM are based essentially on \emph{predictive mean matching} imputation methods (see D'Orazio \emph{et al.} 2006b, Section 2.5 and 3.6).  In particular, the function \code{mixed.mtc} in \pkg{StatMatch} can use two similar mixed methods that manage variables $(X_M, Y, Z)$ following the the multivariate normal distribution.  The main difference is in step (1) when estimating the parameters of the two regressions $Y$ vs. $X_M$  and $Z$ vs. $X_M$.  By default the parameters are estimated through maximum likelihood (argument \code{method="ML"} in \code{mixed.mtc}); in alternative a method proposed by Moriarity and Scheuren (2001, 2003) (argument \code{method="MS"}) is available.  At the end of the step (1), the data set $A$ is filled in with the \dQuote{intermediate} values $\tilde{z}_a=\hat{z}_a+e_a$ $(a=1,\ldots,n_A)$ obtained by adding a random residual term $e_a$ to the predicted values $\hat{z}_a$.  The same happens in $B$ which is filled in with the values $\tilde{y}_b=\hat{y}_b+e_b$ $(b=1,\ldots, n_B)$.  

In the step (2) each record in $A$ is filled in with the value of $Z$ observed on the donor found in $B$ according to a constrained distance hot deck; the Mahalanobis distance is computed by considering the intermediate and live values: couples $\left( y_a,\tilde{z}_a \right)$ in $A$ and $\left( \tilde{y}_b, z_b \right)$ in $B$.

Such a two steps procedure presents various advantages: it offers protection against model misspecification and at the same time reduces the risk of bias in the marginal distribution of the imputed variable because the distances are computed on intermediate and truly observed values of the target value instead of the matching variables $X_M$.  In fact, when computing distances on many matching variables, the variables with low predictive power on the target variable may influence negatively the distances.

D'Orazio \emph{et al.} (2005) compared the two alternative methods based in an extensive simulation study: in general ML tends to perform better, moreover it permits to avoid some incoherencies in the estimation of the parameters that can happen with the Moriarity and Scheuren approach.

In the following example the \code{iris} data set is used just to show how \code{mixed.mtc} works.

<<>>=
# uses iris data set
iris.A <- iris[101:150, 1:3]
iris.B <- iris[1:100, c(1:2,4)]

X.mtc <- c("Sepal.Length","Sepal.Width") # matching variables

# parameters estimated using ML
mix.1 <- mixed.mtc(data.rec=iris.A, data.don=iris.B, match.vars=X.mtc,
                    y.rec="Petal.Length", z.don="Petal.Width", 
                    method="ML", rho.yz=0, 
                    micro=TRUE, constr.alg="Hungarian")

mix.1$mu #estimated means
mix.1$cor #estimated cor. matrix

head(mix.1$filled.rec) # A filled in with Z
cor(mix.1$filled.rec)
@

When using \code{mixed.mtc} the synthetic data set is provided in output as the component \code{filled.rec} of the list returned by calling it with the argument \code{micro=TRUE}.  When \code{micro=FALSE} the function \code{mixed.mtc} returns just the estimates of the parameters (parametric macro approach).

The function \code{mixed.mtc} by default performs mixed SM under the CI assumption ($\rho_{YZ|X_M}=0$ argument \code{rho.yz=0}).  When some additional auxiliary information about the correlation between $Y$ and $Z$ is available (estimates from previous surveys or form external sources) then it can be exploited in SM by specifying a value $(\neq0)$ for the argument \code{rho.yz}; it represents a guess for $\rho_{YZ|X_M}$ when using the ML estimation, or a guess for $\rho_{YZ}$ when estimating the parameters via the Moriarity and Scheuren approach.

<<>>=
# parameters estimated using ML and rho_YZ|X=0.85
mix.2 <- mixed.mtc(data.rec=iris.A, data.don=iris.B, match.vars=X.mtc,
                    y.rec="Petal.Length", z.don="Petal.Width", 
                    method="ML", rho.yz=0.85, 
                    micro=TRUE, constr.alg="Hungarian")
mix.2$cor
head(mix.2$filled.rec)
@

Special attention is required when specifying a guess for $\rho_{YZ}$ under the Moriarity and Scheuren estimation approach (\code{method="MS"}); in particular it may happen that the specified value for $\rho_{YZ}$ is not compatible with the given SM framework (the correlation matrix must be positive semidefinite).  If this is the case, then \code{mixed.mtc} substitutes the input value of \code{rho.yz} by its closest admissible value, as shown in the following example.

<<>>=
mix.3 <- mixed.mtc(data.rec=iris.A, data.don=iris.B, match.vars=X.mtc,
                    y.rec="Petal.Length", z.don="Petal.Width", 
                    method="MS", rho.yz=0.75, 
                    micro=TRUE, constr.alg="Hungarian")

mix.3$rho.yz
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Statistical matching with data from complex surveys
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Statistical matching of data from complex sample surveys} \label{sec:comp_survey}

The SM techniques presented in the previous Sections implicitly or explicitly assume that the observed values in $A$ and $B$ are i.i.d. Unfortunately, when dealing with samples selected from a finite population by means of complex sampling designs (with stratification, clustering, etc.) it is difficult to maintain the i.i.d. assumption: it would mean that the sampling design can be ignored.  If this is not the case, inferences have to account for sampling design and the weights assigned to the units (usually design weights corrected for unit nonresponse, frame errors, etc.) (see S\"arndal \emph{et al.}, 1992, Section 13.6). 

\subsection{Naive micro approaches} \label{sec:comp_survey_naive}

A naive approach to SM of data from complex sample surveys consists in applying nonparametric micro methods (NND, random or rank hot deck) without considering the design nor the units weights.  Once obtained the synthetic dataset (recipient filled in with the missing variables) the successive statistical analyses are carried out by considering the sampling design underlying the recipient data set and the corresponding survey weights.  In the following a simple example applying nearest neighbor hot deck is reported.

<<>>=
# summary info on the weights
sum(samp.A$ww) # estimated pop size from A
sum(samp.B$ww) # estimated pop size from B
summary(samp.A$ww)
summary(samp.B$ww)

# NND constrained hot deck
group.v <- c("sex","area5")
out.nnd <- NND.hotdeck(data.rec=samp.A, data.don=samp.B,
                       match.vars="age", don.class=group.v,
                       dist.fun="Manhattan",
                       constrained=TRUE, constr.alg="Hungarian")

fA.nnd.m <- create.fused(data.rec=samp.A, data.don=samp.B,
                         mtc.ids=out.nnd$mtc.ids,
                         z.vars="labour5")

# estimating distribution of labour5 using weights
t1 <- xtabs(ww~labour5, data=fA.nnd.m) # imputed in A
t2 <- xtabs(ww~labour5, data=samp.B) # ref. estimate in B
c1 <- comp.prop(p1=t1, p2=t2, n1=nrow(fA.nnd.m), ref=TRUE)
c1$meas
@

As far as imputation of missing values is concerned, a way of taking into account the sampling design can be in forming the donation classes by using the design variables (stratification and/or clustering variables) jointly with the most relevant common variables (Andridge and Little, 2010).  Unfortunately in SM this can increase the complexity or may be unfeasible because the design variables may not be available or may be partly available.  Moreover, the two sample surveys may have quite different designs and the design variables used in one survey maybe not available in the other one and vice versa. 

When imputing missing values in a survey, another possibility, consists in using sampling weights (design weights) to form the donation classes (Andridge and Little, 2010).  But again, in SM applications the problem can be slightly more complex even because the sets of weights can be quite different from one survey to the other (usually the available weights are the design weights corrected to compensate for unit nonresponse, to satisfy some given constraints etc.).  The same Authors (Andridge and Little, 2010) indicate that when imputing the missing values, the selection of the donors can be carried out with probability proportional to weights associated to the donors (\emph{weighted random hot deck}).  This feature is implemented in \code{RANDwNDD.hotdeck} when the \code{weight.don} argument to pass the name of the weighting variable.

<<>>=
group.v <- c("sex","area5")
rnd.2 <- RANDwNND.hotdeck(data.rec=samp.A, data.don=samp.B, 
                          match.vars=NULL, don.class=group.v, 
                          weight.don="ww")
fA.wrnd <- create.fused(data.rec=samp.A, data.don=samp.B, 
                        mtc.ids=rnd.2$mtc.ids,
                        z.vars="labour5")

# comparing marginal distribution of labour5 using weights
tt.0w <- xtabs(ww~labour5, data=samp.B)
tt.fw <- xtabs(ww~labour5, data=fA.wrnd)
c1 <- comp.prop(p1=tt.fw, p2=tt.0w, n1=nrow(fA.wrnd), ref=TRUE)
c1$meas
@

The function \code{rankNND.hotdeck}  can use the units' weights ($w_i$)  in estimating the percentage points of the the empirical cumulative distribution function:

$$
\hat{F}(x) = \frac{\sum_{i=1}^n w_i I\left(x_i \leq x \right)}{\sum_{i=1}^n w_i}
$$

In the following it is reported an very simple example with constrained rank hot deck.

<<>>=
rnk.w <- rankNND.hotdeck(data.rec=samp.A, data.don=samp.B, 
                         don.class="area5", var.rec="age", 
                         var.don="age", weight.rec="ww",
                         weight.don="ww", constrained=TRUE,
                         constr.alg="Hungarian")
#
#create the synthetic data set
fA.wrnk <- create.fused(data.rec=samp.A, data.don=samp.B,
                        mtc.ids=rnk.w$mtc.ids, 
                        z.vars="labour5", 
                        dup.x=TRUE, match.vars="age")

# comparing marginal distribution of labour5 using weights
tt.0w <- xtabs(ww~labour5, data=samp.B)
tt.fw <- xtabs(ww~labour5, data=fA.wrnk)
c1 <- comp.prop(p1=tt.fw, p2=tt.0w, n1=nrow(fA.wrnk), ref=TRUE)
c1$meas
@

D'Orazio \emph{et al.} (2012) compared several naive procedures. In general, when rank and random hot deck procedures use the weights, as shown before, they tend to perform quite well in terms of preservation of the marginal distribution of the imputed variable $Z$ and of the joint distribution $X \times Z$ in the synthetic data set. The nearest neighbour donor, performs well only when constrained matching is used and a design variable (used in stratification) is considered in forming donation classes. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Statistical matching method that account explicitly for the sampling weights} \label{sec:ren}

In literature there are few SM methods that explicitly take into account the sampling design and the corresponding sampling weights: Renssen's approach based on weights' \emph{calibrations} (Renssen, 1998); Rubin's \emph{file concatenation} (Rubin, 1986) and the approach based on the empirical likelihood proposed by Wu (2004).  A comparison among these approaches can be found in D'Orazio \emph{et al.} (2010).

The package \pkg{StatMatch} provides functions to apply the procedures suggested by Renssen (1998).  Renssen's approach consists in a series of calibration steps of the survey weights in $A$ and $B$ in order to achieve consistency between estimates (mainly totals) computed separately from the two data sources.   Calibration is a technique very common in sample surveys for deriving new weights, as close as possible to the starting ones, which fulfill a series of constraints concerning totals for a set of auxiliary variables (for further details on calibration see S\"arndal, 2005).  The Renssen's approach works well when dealing with categorical variables or in a mixed case in which the number of continuous variables is very limited.  In the following it will be assumed that all the variables $(X_D,Y,Z)$ are categorical, being $X_D$ a complete or an incomplete crossing of the matching variables $X_M$.  The procedure and the functions developed in \pkg{StatMatch} permits to have one or more continuous variables (better just one) in the subset of the matching variables $X_M$, while $Y$ and $Z$ can be both categorical or a combination of them is allowed ($Y$ categorical and $Z$ continuous, or vice versa).

The first step in the Renssen's procedure consists in calibrating weights in $A$ and in $B$ such that the new weights when applied to the set of the  $X_D$ variables allow to reproduce some known (or estimated) population totals.  In \pkg{StatMatch} the harmonization step can be performed by using \code{harmonize.x}.  This function performs weights calibration (or post-stratification) by means of functions available in the \proglang{R} package \pkg{survey} (Lumley, 2004 and 2014).  When the population totals are already known then they have to be passed to \code{harmonize.x} via the argument \code{x.tot}; on the contrary, when they are unknown (\code{x.tot=NULL}) they are estimated by a weighted average of the totals estimated on the two surveys before the harmonization step:

$$
\tilde{t}_{X_D} = \lambda \hat{t}_{X_D}^{(A)} + \left( 1-\lambda \right) \hat{t}_{X_D}^{(B)}
$$

\noindent being $\lambda= n_A / (n_A+n_B) $ $(n_A$ and $n_B$ are the sample sizes of $A$ and $B$ respectively) (Korn and Graubard, 1999, pp. 281--284).

The following example shows how to harmonize the joint distribution of the gender and classes of age with the data from the previous example, assuming that the joint distribution of age and gender is not known in advance.

<<>>=
tt.A <- xtabs(ww~sex+c.age, data=samp.A)
tt.B <- xtabs(ww~sex+c.age, data=samp.B)
(prop.table(tt.A)-prop.table(tt.B))*100
comp.prop(p1=tt.A, p2=tt.B, n1=nrow(samp.A),
          n2=nrow(samp.B), ref=FALSE)

library(survey, warn.conflicts=FALSE) # loads survey
# creates svydesign objects
svy.samp.A <- svydesign(~1, weights=~ww, data=samp.A)
svy.samp.B <- svydesign(~1, weights=~ww, data=samp.B)
#
# harmonizes wrt to joint distr. of gender vs. c.age
out.hz <- harmonize.x(svy.A=svy.samp.A, svy.B=svy.samp.B,
                      form.x=~c.age:sex-1)
#
summary(out.hz$weights.A) # new calibrated weights for A
summary(out.hz$weights.B) # new calibrated weights for B

tt.A <- xtabs(out.hz$weights.A~sex+c.age, data=samp.A)
tt.B <- xtabs(out.hz$weights.B~sex+c.age, data=samp.B)
c1 <- comp.prop(p1=tt.A, p2=tt.B, n1=nrow(samp.A),
                n2=nrow(samp.B), ref=FALSE)
c1$meas
@

The second step in the Renssen's procedure consists in estimating the the joint distribution $Y$ and $Z$; in practice, when they both are categorical variables of two-way contingency table $Y \times Z$ is the target of estimation; on the contrary, in the mixed case, the objective is the estimation of the total of the continuous variable for each category of the categorical one. When $Y$ and $Z$ are both categorical variables, in absence of auxiliary information, the two-way contingency table $Y \times Z$ is estimated under the CI assumption by means of:

$$
\hat{P}^{(CIA)}_{(Y=j,Z=k)} = \hat{P}^{(A)}_{Y=j|X_D=i} \times \hat{P}^{(B)}_{Z=k|X_D=i} \times \hat{P} _{X_D=i}
$$

\noindent for $i=1,\ldots,I; \ j=1,\ldots,J; \ K=1,\ldots,K$;

In practice, $\hat{P}^{(A)}_{ Y=j|X_D=i}$ is computed from $A$; $\hat{P}^{(B)}_{Z=k|X_D=i}$ is computed from data in $B$ while $P_{X_D=i}$ can be estimated indifferently from $A$ or $B$ (the data set are harmonized with respect to the $X_D$ distribution).  

In \pkg{StatMatch} an estimate of the table $Y \times Z$ under the CIA is provided by the function \code{comb.samples}.


<<>>=
# estimating c.netI vs. labour5 under the CI assumption
out <- comb.samples(svy.A=out.hz$cal.A, svy.B=out.hz$cal.B,
                    svy.C=NULL, y.lab="c.neti", z.lab="labour5",
                    form.x=~c.age:sex-1)
#
addmargins(t(out$yz.CIA))  # table estimated under the CIA
@

When some auxiliary information is available, e.g. a third data source $C$, containing all the variables $(X_M,Y,Z)$ or just $(Y,Z)$, the Renssen's approach permits to exploit it in estimating $Y \times Z$.  Two alternative methods are available: (a) \emph{incomplete two-way stratification}; and (b) \emph{synthetic two-way stratification}.  In practice, both the methods estimate $Y \times Z$ from $C$ after some further calibration steps (for further details see Renssen, 1998).  The function \code{comb.samples} implements both the methods.  In practice, the synthetic two-way stratification (argument \code{estimation="synthetic"}) can be applied only when $C$ contains all the variables of interest $(X_M,Y,Z)$; on the contrary, when the data source $C$ observes just $Y$ and $Z$, only the incomplete two-way stratification method can be applied (argument \code{estimation="incomplete"}).  In the following a simple example is reported based on the artificial EU-SILC data introduced in Section \ref{sec:data}; here a relatively small sample $C$ $(n_C=980)$ with all the variables of interest $(X_M,Y,Z)$ is considered.

<<>>=
data(samp.C, package="StatMatch")
str(samp.C)

#
svy.samp.C <- svydesign(~1, weights=~ww, data=samp.C) 

#
# incomplete two-way estimation
out.inc <- comb.samples(svy.A=out.hz$cal.A, svy.B=out.hz$cal.B,
                        svy.C=svy.samp.C, y.lab="c.neti", 
												z.lab="labour5", form.x=~c.age:sex-1, 
												estimation="incomplete")
addmargins(t(out.inc$yz.est))           
@

The incomplete two-way stratification method estimates the table $Y \times Z$ from $C$ by preserving the marginal distribution of $Y$ and of $Z$ estimated respectively from $A$ and from $B$ after the initial harmonization step; on the contrary, the joint distribution of the matching variables (which is the basis of the harmonization step) is not preserved. 

<<>>=
new.ww <- weights(out.inc$cal.C) #new cal. weights for C 
#
# marginal distributions of c.neti
m.work.cA <- xtabs(out.hz$weights.A~c.neti, data=samp.A)
m.work.cC <- xtabs(new.ww~c.neti, data=samp.C)
m.work.cA-m.work.cC
#
# marginal distributions of labour5
m.cnetI.cB <- xtabs(out.hz$weights.B~labour5, data=samp.B)
m.cnetI.cC <- xtabs(new.ww~labour5, data=samp.C)
m.cnetI.cB-m.cnetI.cC 

# joint distribution of the matching variables
tt.A <- xtabs(out.hz$weights.A~sex+c.age, data=samp.A)
tt.B <- xtabs(out.hz$weights.B~sex+c.age, data=samp.B)
tt.C <- xtabs(new.ww~sex+c.age, data=samp.C)
c1 <- comp.prop(p1=tt.A, p2=tt.B, n1=nrow(samp.A),
                n2=nrow(samp.B), ref=FALSE)
c2 <- comp.prop(p1=tt.C, p2=tt.A, n1=nrow(samp.C),
                n2=nrow(samp.A), ref=FALSE)
c1$meas
c2$meas
@

As said before, the synthetic two-way stratification (argument \code{estimation="synthetic"}) requires that the auxiliary data source $C$ contains the matching variables $X_M$ and the target variables $Y$ and $Z$.

<<>>=
# synthetic two-way estimation
out.synt <- comb.samples(svy.A=out.hz$cal.A, svy.B=out.hz$cal.B,
                         svy.C=svy.samp.C, y.lab="c.neti", 
												 z.lab="labour5", form.x=~c.age:sex-1, 
												 estimation="synthetic")
#
addmargins(t(out.synt$yz.est))           
@

As in the case of incomplete two-way stratification, also the synthetic two-way stratification derives the table $Y \times Z$ from $C$ by preserving the marginal distribution of $Y$ and of $Z$ estimated respectively from $A$ and from $B$ after the initial harmonization step; on the contrary, the joint distribution of the matching variables (which is the basis of the harmonization step) is still not preserved. 

It is worth noting that \code{comb.samples} can also be used for micro imputation.  In particular, when the argument \code{micro} is set to \code{TRUE} the function returns also the two data frames \code{Z.A} and \code{Y.B}. The first ones has the same rows as \code{svy.A} and predicted values for the $Z$ variables; note that when $Z$ is categorical then \code{Z.A} will have a number of columns equals the number of categories of the $Z$ variable (specified via \code{z.lab}); in this case, each row provides the estimated probabilities for a unit of assuming a value in the various categories. The same happens for \code{Y.B} that with a categorical $Y$ variable will provide the estimated probabilities of assuming a category of \code{y.lab} for each unit in $B$. 
The predictions are obtained as a by-product of the whole procedure which is based on the usage of the \emph{linear probability models} (for major details see Renssen, 1998).  The procedure corresponds to a regression imputation that, when dealing with all categorical variables ($X_D, Y, Z$), provides a synthetic data set ($A$ filled in with $Z$) which preserves the marginal distribution of the $Z$ variable and the joint distribution $X \times Z$.  Unfortunately, linear probability models have some well known drawbacks and may provide estimated probabilities less than 0 or greater than 1.  For this reason, such predictions should be used carefully.  


<<>>=
# predicting prob of labour5 in A under the CI assumption
out <- comb.samples(svy.A=out.hz$cal.A, svy.B=out.hz$cal.B,
                    svy.C=NULL, y.lab="c.neti", z.lab="labour5",
                    form.x=~c.age:sex-1, micro=TRUE)
head(out$Z.A)
sum(out$Z.A<0) # negative est. prob.
sum(out$Z.A>1) # est. prob. >1

# compare marginal distributions of Z
t.zA <- colSums(out$Z.A * out.hz$weights.A)
t.zB <- xtabs(out.hz$weights.B ~ samp.B$labour5)
c1 <- comp.prop(p1=t.zA, p2=t.zB, n1=nrow(samp.A), ref=TRUE)  
c1$meas
@

D'orazio \emph{et al.} (2012) suggest using a randomization mechanism to derive the predicted category starting from the estimated probabilities.

<<>>=
# predicting categories of labour5 in A
# randomized prediction with prob proportional to estimated prob.
pps1 <- function(x) sample(x=1:length(x), size=1, prob=x)
pred.zA <- apply(out$Z.A, 1, pps1)
samp.A$labour5 <- factor(pred.zA, levels=1:nlevels(samp.B$labour5), 
                       labels=as.character(levels(samp.B$labour5)), 
                       ordered=T)

# comparing marginal distributions of Z
t.zA <- xtabs(out.hz$weights.A ~ samp.A$labour5)
c1 <- comp.prop(p1=t.zA, p2=t.zB, n1=nrow(samp.A), ref=TRUE)  
c1$meas

# comparing joint distributions of X vs. Z
t.xzA <- xtabs(out.hz$weights.A~c.age+sex+labour5, data=samp.A)
t.xzB <- xtabs(out.hz$weights.B~c.age+sex+labour5, data=samp.B)
out.comp <- comp.prop(p1=t.xzA, p2=t.xzB, n1=nrow(samp.A), ref=TRUE)  
out.comp$meas
out.comp$chi.sq
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% uncertainty

\section{Exploring uncertainty due to the statistical matching framework} \label{sec:unc}

When the objective of SM consists in estimating a parameter (macro approach) it is possible to tackle SM in an alternative way consisting in the \dQuote{exploration} of the uncertainty on the model chosen for $(X_M,Y,Z)$, due to the lack of knowledge typical of the basic SM framework (no auxiliary information is available).  This approach does not end with a unique estimate of the unknown parameter characterizing the joint p.d.f. for $(X_D,Y,Z)$; on the contrary it identifies an interval of plausible values for it.  When dealing with categorical variables, the estimation of the intervals of plausible values for the probabilities in the table $Y \times Z$ are provided by the Fr\'echet bounds:

$$
\max\{0; P_{Y=j} + P_{Z=k} - 1\} \ \leq \ P_{Y=j,Z=k} \ \leq \ \min \{P_{Y=j}; P_{Z=k}\}
$$

\noindent for $j=1,\ldots, J$ and $k=1,\ldots, K$, being $J$ and $K$ the categories of $Y$ and $Z$ respectively.

Let consider the matching variables $X_M$, for sake of simplicity let assume that $X_D$ is the variable obtained by the crossproduct of the chosen $X_M$ variables; by conditioning on $X_D$, it is possible to derive the following result (D'Orazio \emph{et al.}, 2006a):

$$
P^{(low)}_{j,k} \ \leq \ P_{Y=j,Z=k} \ \leq \ P^{(up)}_{j,k}
$$

with

\begin{eqnarray}
P^{(low)}_{j,k} &=& \sum_{i}  P_{X_D=i} \times \max \left\{ 0; P_{Y=j|X_D=i} + P_{Z=k|X_D=i} - 1 \right\} \nonumber\\
 P^{(up)}_{j,k} &=& \sum_{i}  P_{X_D=i} \times \min \left\{ P_{Y=j|X_D=i}; P_{Z=k|X_D=i} \right\} \nonumber
\end{eqnarray}

\noindent for $j=1,\ldots, J$ and $k=1,\ldots, K$.
It is interesting to observe that the CIA estimate of $P_{Y=j,Z=k}$ is always included in the interval identified by such bounds:

$$
P^{(low)}_{j,k} \ \leq \ P_{Y=j,Z=k}^{(CIA)} \ \leq \ P^{(up)}_{j,k}
$$

In the SM basic framework, the probabilities $P_{Y=j|X_D=i}$ are estimated from $A$, the $P_{Z=k|X_D=i}$ are estimated from $B$, while the marginal distribution $P_{X_D=i}$ can be estimated indifferently on $A$ or on $B$, assuming that both the samples, being representative samples of the same population, provide not significantly different estimates of $P(X_M=i)$.  If this is not the case, before computing the bounds it would be preferable to harmonize the distribution of $X_D$ in $A$ and in $B$ by using the function \code{harmonize.x}.

In \pkg{StatMatch} the Fr\'echet bounds for $P_{Y=j,Z=k}$ ($j=1,\ldots, J$ and $k=1,\ldots, K$), conditioned or not on $X_D$, are provided by \code{Frechet.bounds.cat}.

<<>>=
#comparing joint distribution of the X_M variables in A and in B
t.xA <- xtabs(ww~c.age+sex, data=samp.A)
t.xB <- xtabs(ww~c.age+sex, data=samp.B)
comp.prop(p1=t.xA, p2=t.xB, n1=nrow(samp.A), n2=nrow(samp.B), ref=FALSE)
#
#computing tables needed by Frechet.bounds.cat
t.xy <- xtabs(ww~c.age+sex+c.neti, data=samp.A)
t.xz <- xtabs(ww~c.age+sex+labour5, data=samp.B)
out.fb <- Frechet.bounds.cat(tab.x=t.xA, tab.xy=t.xy, tab.xz=t.xz, 
                             print.f="data.frame")
out.fb
@

The final component of the output list provided by \code{Frechet.bounds.cat} summarizes the uncertainty by means of the average width of the unconditioned bounds and the average width of the bounds obtained by conditioning on $X_D$ %and the overall uncertainty measured as suggested by Conti \emph{et al.} (2012) (see Section \ref{sec:mtc_vars}).
Please note the it would be preferable to derive the uncertainty bounds after the harmonization of the joint distribution of the $X_D$ variables in the source data sets.

When dealing with continuous variables, if it is assumed that their joint distribution is multivariate normal, the uncertainty bounds for the correlation coefficient $\rho_{YZ}$ can be obtained by using the function \code{mixed.mtc} with argument \code{method="MS"}.  The following example assumes multivariate normal distribution holding for joint distribution for age, gender (the matching variables), the log-transformed personal net income (log of \code{"netIncome"} which plays the role of $Y$) and the aggregated personal economic status (binary variable \code{"work"} which plays the role of $Z$).

<<>>=
# continuous variables
samp.A$log.netI <- log(ifelse(samp.A$n.income>0, samp.A$n.income, 0) + 1)
lab <- as.integer(samp.B$labour5)
samp.B$work <- factor(ifelse(lab<3, 1, 2)) # binary variable working status

X.mtc <- c("age", "sex")
mix.3 <- mixed.mtc(data.rec=samp.A, data.don=samp.B, match.vars=X.mtc,
                   y.rec="log.netI", z.don="work", 
                   method="MS")
@ 

When a single $X$ variable is considered, the bounds can be obtained explicitly by using formula in Section \ref{sec:intro}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Acknowledgments}
%This work was developed in the framework of the ESSnet project on Data Integration partly funded by the Eurostat (December 2009--December 2011).  For more information on the project visit \url{http://www.essnet-portal.eu/di/data-integration}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ------------
% bibliography
% ------------

\section*{References} \label{ref} 

\small
\begin{flushleft}

\begin{description}
 \setlength{\itemsep}{0pt}

\item[] Andridge R.R., Little R.J.A. (2009) \dQuote{The Use of Sample Weights in Hot Deck Imputation}. \emph{Journal of Official Statistics}, \textbf{25}(1), 21--36.

\item[] Andridge R.R., Little R.J.A. (2010) \dQuote{A Review of Hot Deck Imputation for Survey Nonresponse}. \emph{International Statistical Review}, \textbf{78}, 40--64.

\item[] Berkelaar M. and others (2015) \dQuote{lpSolve: Interface to Lpsolve v. 5.5 to solve linear--integer programs}. R package version 5.6.13. \url{http://CRAN.R-project.org/package=lpSolve}
  
\item[] Breiman, L. (2001) \dQuote{Random Forests}, \emph{Machine Learning}, \textbf{45}(1), 5--32.

\item[] Breiman L., Friedman J. H., Olshen R. A., and Stone, C. J. (1984) \emph{Classification and Regression Trees}. Wadsworth. 

\item[] Cohen M.L. (1991) \dQuote{Statistical matching and microsimulation models}, in Citro and Hanushek (eds) \emph{Improving Information for Social Policy Decisions: The Uses of Microsimulation Modeling. Vol II Technical papers}. Washington D.C.

%\item[] Conti, P.L., Marella, D., and Scanu, M. (2012) \dQuote{Uncertainty Analysis in Statistical Matching}, \emph{Journal of Official Statistics}, \textbf{28}, 69--88.

\item[] D'Orazio M. (2010) \dQuote{Statistical matching when dealing with data from complex survey sampling}, in \emph{Report of WP1. State of the art on statistical methodologies for data integration}, ESSnet project on Data Integration, 33--37, \url{http://www.essnet-portal.eu/sites/default/files/131/ESSnetDI_WP1_v1.32.pdf}

\item[] D'Orazio M. (2016) \dQuote{StatMatch: Statistical Matching}. R package version 1.2.5. 
\url{http://CRAN.R-project.org/package=StatMatch}

\item[] D'Orazio M., Di Zio M., Scanu, M. (2005) \dQuote{A comparison among different estimators of regression parameters on statistically matched files trough an extensive simulation study}. \emph{Contributi Istat}, \textbf{2005/10}

\item[] D'Orazio M., Di Zio M., Scanu M. (2006a) \dQuote{Statistical matching for categorical data: Displaying uncertainty and using logical constraints}. \emph{Journal of Official Statistics } {\textbf 22}, 137--157.

\item[] D'Orazio M., Di Zio M., Scanu M. (2006b) \emph{Statistical matching: Theory and practice}. Wiley, Chichester.

\item[] D'Orazio M., Di Zio M., Scanu M. (2008) \dQuote{The statistical matching workflow}, in: \emph{Report of WP1: State of the art on statistical methodologies for integration of surveys and administrative data}, \dQuote{ESSnet Statistical Methodology Project on Integration of Survey and Administrative Data}, 25--26. \url{http://cenex-isad.istat.it/}

\item[] D'Orazio M., Di Zio M., Scanu M. (2010) \dQuote{Old and new approaches in statistical matching when samples are drawn with complex survey designs}. \emph{Proceedings of the 45th \dQuote{Riunione Scientifica della Societa' Italiana di Statistica}}, Padova 16--18 June 2010.

\item[] D'Orazio M., Di Zio M., Scanu M. (2012) \dQuote{Statistical Matching of Data from Complex Sample Surveys}. \emph{Proceedings of the European Conference on Quality in Official Statistics - Q2012}, 29 May--1 June 2012, Athens, Greece.

\item[] Gower J. C. (1971) \dQuote{A general coefficient of similarity and some of its properties}. \emph{Biometrics}, \textbf{27}, 623--637.

\item[] Harrell F.E. (2001) \emph{Regression Modeling Strategies With Applications to Linear Models, Logistic Regression, and Survival Analysis}. New York, Springer

\item[] Harrell F.E., with contributions from Charles Dupont and many others (2016). \dQuote{Hmisc: Harrell Miscellaneous}. R package version 4.0-1. \url{https://CRAN.R-project.org/package=Hmisc}

\item[] Hornik K. (2005).  \dQuote{A CLUE for CLUster Ensembles}.
  \emph{Journal of Statistical Software} 14/12.  \url{
  http://www.jstatsoft.org/v14/i12/}.

\item[] Hornik K. (2014).  \dQuote{clue: Cluster ensembles}.  R package version 0.3-48.  
  \url{http://CRAN.R-project.org/package=clue}.

\item[] Korn E.L., Graubard B.I. (1999) \emph{Analysis of Health Surveys}. Wiley, New York

\item[] Kovar J.G., MacMillan J., Whitridge P. (1988) \dQuote{Overview and strategy for the Generalized Edit and Imputation System}. Statistics Canada, Methodology Working Paper, No. BSMD 88-007 E/F.

\item[] Liaw A., Wiener M. (2002) \dQuote{Classification and Regression by randomForest}. \emph{R News}, \textbf{2}(3), 18--22.

\item[] Lumley T. (2004) \dQuote{Analysis of complex survey samples}.
  \emph{Journal of Statistical Software}, 9(1): 1-19

\item[] Lumley T. (2016) \dQuote{survey: analysis of complex survey samples}. R package version 3.31-5. \url{http://CRAN.R-project.org/package=survey}

\item[] Meyer D., Buchta C. (2016) \dQuote{proxy: Distance and Similarity Measures}. R package version 0.4-16.  \url{http://CRAN.R-project.org/package=proxy}

\item[] Moriarity C., Scheuren F. (2001) \dQuote{Statistical matching: a paradigm for assessing the uncertainty in the procedure}. \emph{Journal of Official Statistics}, \textbf{17}, 407--422. 

\item[] Moriarity C., Scheuren F. (2003). \dQuote{A note on Rubin's statistical matching using file concatenation with adjusted weights and multiple imputation}, \emph{Jour. of Business and Economic Statistics}, \textbf{21}, 65--73.

\item[] R Core Team (2016) \emph{R: A language and environment for statistical computing}. R Foundation for Statistical Computing, Vienna, Austria.  \url{http://www.R-project.org/}.

\item[] R\"assler S. (2002) \emph{Statistical matching: a frequentist theory, practical applications and alternative Bayesian approaches}. Springer Verlag, New York.

\item[] Renssen R.H.(1998) \dQuote{Use of statistical matching techniques in calibration estimation}. \emph{Survey Methodology} {\textbf 24}, 171--183.

\item[] Rubin D.B. (1986) \dQuote{Statistical matching using file concatenation with adjusted weights and multiple imputations}. \emph{Journal of Business and Economic Statistics}, {\textbf 4}, 87--94.

\item[] S\"arndal C.E., Swensson B., Wretman J. (1992) \emph{Model Assisted Survey Sampling}. Springer-Verlag, New York.

\item[] S\"arndal C.E., Lundstr\"om S. (2005) \emph{Estimation in Surveys with Nonresponse}. Wiley, New York.

\item[] Scanu M. (2008) \dQuote{The practical aspects to be considered for statistical matching}. in: \emph{Report of WP2: Recommendations on the use of methodologies for the integration of surveys and administrative data}, \dQuote{ESSnet Statistical Methodology Project on Integration of Survey and Administrative Data}, 34--35. \url{http://cenex-isad.istat.it/}

\item[] Singh A.C., Mantel H., Kinack M., Rowe G. (1993) \dQuote{Statistical matching: use of auxiliary information as an alternative to the conditional independence assumption}. \emph{Survey Methodology}, \textbf{19}, 59--79.

\item[] Wu C. (2004) \dQuote{Combining information from multiple surveys through the empirical likelihood method}. \emph{The Canadian Journal of Statistics}, \textbf{32}, 15--26.

\end{description}
\end{flushleft}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
